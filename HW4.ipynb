{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# HW4: Neural Network\n",
    "\n",
    "(Cập nhật lần cuối: 10/04/2023)\n",
    "\n",
    "Họ tên: Trần Đàm Gia Huy\n",
    "\n",
    "MSSV: 20127039"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nắm cách làm bài và nộp bài"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9889; Bạn lưu ý là mình sẽ dùng chương trình hỗ trợ chấm bài nên bạn cần phải tuân thủ chính xác qui định mà mình đặt ra, nếu không rõ thì hỏi, chứ không nên tự tiện làm theo ý của cá nhân.\n",
    "\n",
    "**Cách làm bài**\n",
    "\n",
    "Bạn sẽ làm trực tiếp trên file notebook này. Đầu tiên, bạn điền họ tên và MSSV vào phần đầu file ở bên trên. Trong file, bạn làm bài ở những chỗ có ghi là:\n",
    "```python\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "```\n",
    "hoặc đối với những phần code không bắt buộc thì là:\n",
    "```python\n",
    "# YOUR CODE HERE (OPTION)\n",
    "```\n",
    "hoặc đối với markdown cell thì là:\n",
    "```markdown\n",
    "YOUR ANSWER HERE\n",
    "```\n",
    "Tất nhiên, khi làm thì bạn xóa dòng `raise NotImplementedError()` đi.\n",
    "Đối những phần yêu cầu code thì thường ở ngay phía dưới sẽ có một (hoặc một số) cell chứa các bộ test để phần nào giúp bạn biết đã code đúng hay chưa; nếu chạy cell này không có lỗi gì thì có nghĩa là qua được các bộ test. Trong một số trường hợp, các bộ test có thể sẽ không đầy đủ; nghĩa là, nếu không qua được test thì là code sai, nhưng nếu qua được test thì chưa chắc đã đúng hoàn toàn.\n",
    "\n",
    "Trong khi làm bài, bạn có thể cho in ra màn hình, tạo thêm các cell để test. Nhưng khi nộp bài thì bạn xóa các cell mà bạn tự tạo, xóa hoặc comment các câu lệnh in ra màn hình. Bạn lưu ý <font color=red>không được tự tiện xóa các cell hay sửa code của Thầy</font> (trừ những chỗ được phép sửa như đã nói ở trên).\n",
    "\n",
    "Trong khi làm bài, thường xuyên `Ctrl + S` để lưu lại bài làm của bạn, tránh mất mát thông tin.\n",
    "\n",
    "*Nên nhớ mục tiêu chính ở đây là <font color=green>học, học một cách chân thật</font>.  Bạn có thể thảo luận ý tưởng với bạn khác cũng như tham khảo các nguồn trên mạng, nhưng sau cùng <font color=green>code và bài làm phải là của bạn, dựa trên sự hiểu thật sự của bạn</font> (khi tham khảo các nguồn trên mạng thì bạn cần ghi rõ nguồn trong bài làm, và đương nhiên là bạn cũng không được phép đưa code và bài làm cho bạn khác xem). <font color=red>Nếu vi phạm những điều này thì có thể bạn sẽ bị 0 điểm cho toàn bộ môn học.</font>*\n",
    "\n",
    "**Cách nộp bài**\n",
    "\n",
    "Khi chấm bài, đầu tiên mình sẽ chọn `Kernel` - `Restart Kernel & Run All Cells`, để restart và chạy tất cả các cell trong notebook của bạn; do đó, trước khi nộp bài, bạn nên chạy thử `Kernel` - `Restart Kernel & Run All Cells` để đảm bảo mọi chuyện diễn ra đúng như mong đợi.\n",
    "\n",
    "Sau đó, bạn tạo thư mục nộp bài theo cấu trúc sau:\n",
    "- Thư mục `MSSV` (vd, nếu bạn có MSSV là 1234567 thì bạn đặt tên thư mục là `1234567`)\n",
    "    - File `HW4.ipynb` (không cần nộp các file khác)\n",
    "\n",
    "Cuối cùng, bạn nén thư mục `MSSV` này lại với định dạng nén là .zip (chứ không được là .rar hay các định dạng khác) và nộp ở link trên moodle. \\\n",
    "<font color=red>Bạn lưu ý tuân thủ chính xác qui định nộp bài này.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kiểm tra môi trường code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bạn nên thấy kết quả in ra là đường dẫn đến file chạy python của môi trường \"min_ml-env\" mà mình đã hướng dẫn bạn cài đặt ở HW0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nắm bài toán cần giải quyết của bài tập này\n",
    "\n",
    "Cho dữ liệu quan sát được (đây là dữ liệu huấn luyện gốc, dữ liệu này thường sẽ được tách ra một phần làm dữ liệu validation): \n",
    "$$\\{(\\textbf{x}^{(1)}, y^{(1)}), ..., (\\textbf{x}^{(N_{train})}, y^{(N_{train})})\\}$$\n",
    "\n",
    "Trong đó:\n",
    "\n",
    "- $\\textbf{x}^{(n)} \\in \\mathbb{R}^{784}$ là véc-tơ đầu vào chứa các giá trị pixel của một ảnh xám $28\\times28$, ảnh này là ảnh một chữ số viết tay nào đó (véc-tơ $784$ chiều được tạo từ ảnh xám $28\\times28$ bằng cách nối các dòng của ảnh xám lại với nhau)\n",
    "- $y^{(n)} \\in \\{0, 1, ..., 9\\}$ là đầu ra tương ứng, cho biết đây là chữ số nào\n",
    "\n",
    "Nhiệm vụ ở đây là tìm ra một mô hình Neural Network (cùng cách tiền xử lý nếu có) từ dữ liệu này sao cho mô hình Neural Network này (cùng cách tiền xử lý nếu có) có thể nhận đầu vào là một ảnh-chữ-số-viết-tay *mới* (là một véc-tơ $\\in \\mathbb{R}^{784}$) và dự đoán đầu ra tương ứng (chữ số nào trong $\\{0, 1, 2, ..., 9\\}$) một cách *chính xác*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dưới đây là một số tài liệu lý thuyết về mô hình Neural Network mà bạn nên đọc/xem trước khi bắt đầu làm (lưu ý: các tài liệu khác nhau có thể dùng các ký hiệu khác nhau):\n",
    "- [Slide về Neural Network của môn học](https://drive.google.com/drive/folders/1KDifaACbAuomUiyBsiZfN09gFoKa5iqo) (file \"lect12...\")\n",
    "- [Slide về Neural Network của mình](https://drive.google.com/file/d/1IH7nXTR-0kVxWyyDlNorlxBAyl0eWECb/view?usp=sharing)\n",
    "- [Các video về Neural Network của 3B1B](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) (mình thấy rất trực quan và \"inspirational\")\n",
    "\n",
    "Bạn đã sẵn sàng chưa? Hãy bắt đầu thôi!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import các thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot') # Để hình vẽ đẹp hơn một xíu ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lưu ý: khi tính toán với mảng Numpy, bạn nên dùng các toán-tử/hàm/phương-thức mà Numpy đã cũng cung cấp sẵn. Các toán-tử/hàm/phương-thức này làm trên nguyên mảng và ở bên dưới đã được tối ưu hóa; do đó, code sẽ ngắn gọn và chạy nhanh. Nếu bạn dùng vòng lặp for và làm với từng phần tử của mảng Numpy thì code sẽ dài và chạy chậm $\\to$ bạn sẽ bị trừ điểm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Đọc dữ liệu (giống HW2 & HW3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bộ dữ liệu mà ta sẽ dùng trong bài này là MNIST - bộ dữ liệu chữ số viết tay \"nổi tiếng\" trong cộng đồng làm Machine Learning. [Bộ MNIST gốc](http://yann.lecun.com/exdb/mnist/) gồm có: dữ liệu huấn luyện (60000 ảnh) và dữ liệu kiểm tra (10000 ảnh). Bộ MNIST mà ta sẽ dùng trong bài này (file \"mnist.pkl.gz\") gồm có: dữ liệu huấn luyện (50000 ảnh), dữ liệu validation (10000 ảnh), và dữ liệu kiểm tra (10000 ảnh); dữ liệu huấn luyện và validation ở đây được tạo ra bằng cách tách dữ luyện huấn luyện gốc ra thành 2 phần theo tỉ lệ 5:1. \n",
    "\n",
    "Về mặt ý nghĩa thì dữ liệu validation và dữ liệu kiểm tra đều là dữ liệu mới ngoài dữ liệu huấn luyện. Dữ liệu validation giống như đề thi thử, có thể được thi một vài lần; còn dữ liệu kiểm tra giống như đề thi thật, để đảm bảo kết quả được khách quan thì chỉ được thi một lần duy nhất! Khi làm Machine Learning, ta thường muốn thử một số cách tiền xử lý + mô hình để chọn ra cách tiền xử lý + mô hình tốt nhất. Với mỗi cách tiền xử lý + mô hình, ta sẽ huấn luyện trên dữ liệu huấn luyện và đo độ lỗi dự đoán trên dữ liệu validation; cuối cùng ta sẽ chọn cách tiền xử lý + mô hình mà có độ lỗi dự đoán thấp nhất trên dữ liệu validation (ta không chọn dựa vào độ lỗi dự đoán trên dữ liệu huấn luyện vì có thể xảy ra trường hợp \"học vẹt\": cách tiền xử lý + mô hình có độ lỗi rất thấp trên dữ liệu huấn luyện nhưng lại có độ lỗi cao với dữ liệu mới ngoài dữ liệu huấn luyện). Khi đã chọn xong cách tiền xử lý + mô hình rồi thì ta sẽ đo một lần duy nhất độ lỗi dự đoán trên dữ liệu kiểm tra để có một ước lượng khách quan về độ lỗi thật sự! Nếu bạn nhìn vào độ lỗi dự đoán trên dữ liệu kiểm tra và quay lại điều chỉnh cách tiền xử lý + mô hình thì kết quả trên dữ liệu kiểm tra sẽ không còn sự khách quan nữa!\n",
    "\n",
    "Đoạn code dưới đây sẽ đọc dữ liệu từ file \"mnist.pkl.gz\" và lưu kết quả vào 6 mảng:\n",
    "\n",
    "- `train_X`, `train_y`\n",
    "- `val_X`, `val_y`\n",
    "- `test_X`, `test_y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mnist(mnist_file):\n",
    "    if os.path.isfile(mnist_file) == False:\n",
    "        mnist_file = os.path.join(os.path.expanduser('~'), 'data', 'mnist.pkl.gz')\n",
    "    \n",
    "    f = gzip.open(mnist_file, 'rb')\n",
    "    train_data, val_data, test_data = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "    \n",
    "    train_X, train_Y = train_data\n",
    "    val_X, val_Y = val_data\n",
    "    test_X, test_Y = test_data    \n",
    "    \n",
    "    return train_X, train_Y, val_X, val_Y, test_X, test_Y\n",
    "\n",
    "# Bạn cần đặt file \"mnist.pkl.gz\" vào cùng thư mục với file notebook này,\n",
    "# hoặc bạn cũng có thể đặt ở thư mục tương ứng với câu lệnh này:\n",
    "# os.path.join(os.path.expanduser('~'), 'data')\n",
    "train_X, train_y, val_X, val_y, test_X, test_y = read_mnist('mnist.pkl.gz')\n",
    "print(f'Shape of train_X: {train_X.shape}, shape of train_y: {train_y.shape}')\n",
    "print(f'Shape of val_X:   {val_X.shape}, shape of val_y:   {val_y.shape}')\n",
    "print(f'Shape of test_X:  {test_X.shape}, shape of test_y:  {test_y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Khám phá dữ liệu huấn luyện (giống HW2 & HW3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đầu tiên, ta hãy xem thử min và max của `train_X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Min of train_X: {train_X.min()}, max of train_X: {train_X.max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Với ảnh xám thì giá trị của mỗi pixel thường sẽ nằm trong đoạn [0, 255] (với 0 là màu đen và 255 là màu trắng), hoặc đôi khi được chuẩn hóa về đoạn [0, 1] (với 0 là màu đen và 1 là màu trắng). Ở đây có vẻ giá trị pixel của ảnh xám của ta nằm trong đoạn [0, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiếp theo, ta hãy thử xem mặt mũi của vài ảnh trong `train_X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bạn có thể chạy cell này nhiều lần để xem các ảnh ngẫu nhiên khác nhau\n",
    "n_rimages = 10; n_cimages = 10 \n",
    "padding = 2 \n",
    "canvas = 0.5 * np.ones((n_rimages * (28 + 2 * padding), n_cimages * (28 + 2 * padding)))\n",
    "rand_idxs = np.random.permutation(np.arange(len(train_X))[:n_rimages * n_cimages])\n",
    "for r in range(n_rimages):\n",
    "    for c in range(n_cimages):\n",
    "        i = r * n_cimages + c\n",
    "        image = train_X[rand_idxs[i]].reshape(28, 28)\n",
    "        temp1 = r * (28 + 2 * padding) + padding \n",
    "        temp2 = c * (28 + 2 * padding) + padding \n",
    "        canvas[temp1:temp1 + 28, temp2:temp2 + 28] = image\n",
    "plt.imshow(canvas, cmap='gray', vmin=0, vmax=1)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiếp theo, ta hãy xem các giá trị có thể có của `train_y` và số lượng của mỗi giá trị này."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, counts = np.unique(train_y, return_counts=True)\n",
    "for value, count in zip(values, counts):\n",
    "    print(f'Value: {value}, count: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như vậy là `train_y` có 10 giá trị có thể có ứng với 10 chữ số từ 0 đến 9. Và số lượng ảnh của mỗi chữ số cũng khá tương đương nhau. Tốt ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiền xử lý dữ liệu huấn luyện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_ones(X):\n",
    "    return np.hstack((np.ones((len(X), 1)), X))\n",
    "\n",
    "# Gọi hàm add_ones để tiền xử lý train_X\n",
    "train_Z = add_ones(train_X)\n",
    "train_Z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tìm mô hình Neural Network từ dữ liệu huấn luyện"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong bài này, ta sẽ làm với một kiến trúc đơn giản của mô hình Neural Network: Fully-Connected Feed-Forward Neural Network. Ta sẽ dùng hàm kích hoạt sigmoid ở tầng ẩn và hàm softmax ở tầng đầu ra. Như vậy, có thể xem mô hình Neural Network của ta là mô hình Softmax Regression mà có véc-tơ đầu vào là $\\textbf{z}$ với $\\textbf{z}$ được tính từ véc-tơ đầu vào ban đầu $\\textbf{x}$ thông qua các tầng ẩn. Ý tưởng lớn của mô hình Neural Network là mô hình sẽ tự động học luôn cách xác định véc-tơ đầu vào $\\textbf{z}$ từ dữ liệu, thay vì con người phải ngồi suy nghĩ cách thiết kế $\\textbf{z}$. Ưu điểm của cách làm này của mô hình Neural Network là con người có thể để cho máy tính chạy và đi uống coffee :-). Nhưng nhược điểm là mô hình sẽ có nhiều tham số (trọng số) cần phải học hơn, và do đó sẽ cần nhiều dữ liệu hơn để có thể học tốt; điều này cũng có nghĩa là sẽ cần máy tính mạnh hơn, huấn luyện lâu hơn, tốn tiền điện nhiều hơn, thải CO2 ra môi trường nhiều hơn :-(. Một nhược điểm nữa là con người có thể không hiểu được véc-tơ đầu vào $\\textbf{z}$ mà mô hình đưa ra. Về độ chính xác dự đoán, cho đến thời điểm hiện tại mô hình Neural Network là mô hình đạt được độ chính xác dự đoán rất tốt với dữ liệu hình ảnh, âm thanh, văn bản."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quay trở lại việc cài đặt mô hình Neural Network trong bài này, nhiệm vụ 1 của bạn (2đ): viết hàm `compute_nnet_output`. Hàm này sẽ được dùng trong hàm `train_nnet` ở bên dưới; ngoài ra, sau khi huấn luyện xong thì hàm này sẽ được dùng để dự đoán các đầu ra của các véc-tơ đầu vào mới.\n",
    "\n",
    "Hàm `compute_nnet_output` có các tham số đầu vào:\n",
    "- `Ws`: list chứa các mảng trọng số của mô hình Neural Network. `Ws[l-1]` là mảng trọng số của tầng `l` với `l` >= 1 (tầng 0 là tầng đầu vào, không có mảng trọng số); mảng trọng số của tầng `l` có shape là <font color=blue>(</font>1 + số lượng nơ-ron của tầng `l-1`<font color=blue>,</font> số lượng nơ-ron của tầng `l`<font color=blue>)</font> với \"số lượng nơ-ron của tầng\" là không tính nơ-ron mà luôn có giá trị đầu ra là 1\n",
    "- `X`: mảng chứa các véc-tơ đầu vào cần dự đoán (đã được thêm 1 ở đầu), mảng này có shape là (`N`, `d+1`) với `N` là số lượng các véc-tơ đầu vào và `d` là số lượng phần tử của mỗi véc-tơ đầu vào (khi chưa thêm 1 ở đầu)\n",
    "- `return_what`: nếu tham số này bằng `all` thì sẽ trả về list chứa các mảng-chứa-các-véc-tơ-đầu-ra ở tất cả các tầng (ta sẽ cần dùng list này trong quá trình huấn luyện); nếu tham số này bằng `prob` thì sẽ trả về mảng chứa các véc-tơ đầu ra ở tầng đầu ra, mỗi véc-tơ đầu ra cho biết xác suất các lớp của véc-tơ đầu vào tương ứng; nếu tham số này bằng `class` hoặc các giá trị khác thì sẽ trả về mảng chứa các lớp đầu ra dự đoán, lớp đầu ra dự đoán của một véc-tơ đầu vào là lớp mà có xác suất lớn nhất trong véc-tơ đầu ra chứa xác suất các lớp\n",
    "\n",
    "Hàm `compute_nnet_output` sẽ trả về:\n",
    "\n",
    "- Nếu `return_what` bằng `all` thì sẽ trả về list chứa các mảng-chứa-các-véc-tơ-đầu-ra ở tất cả các tầng, bao gồm cả tầng 0 (tầng đầu vào). Phần tử chỉ số `i` của list này là mảng chứa các véc-tơ đầu ra của tầng `i` tương ứng với các véc-tơ đầu vào của `X`; mảng này có shape là (`N`, 1 + số lượng nơ-ron của tầng `i`) nếu mảng này không phải là phần tử cuối của list (\"1 + ...\" là ứng với giá trị 1 của nơ-ron mà luôn có giá trị đầu ra là 1), còn nếu mảng này là phần tử cuối của list (tầng đầu ra) thì sẽ có shape là (`N`, số lượng nơ-ron của tầng `i`). Nhắc lại: \"số lượng nơ-ron của tầng\" là không tính nơ-ron mà luôn có giá trị đầu ra là 1, `N` là số lượng véc-tơ đầu vào của `X`\n",
    "- Nếu `return_what` bằng `prob` thì sẽ trả về mảng chứa các véc-tơ đầu ra ở tầng đầu ra mà tương ứng với các véc-tơ đầu vào của `X`, mỗi véc-tơ đầu ra chứa xác suất các lớp của véc-tơ đầu vào tương ứng; mảng này có shape là (`N`, số lượng lớp)\n",
    "- Nếu `return_what` bằng `class` hoặc các giá trị khác thì sẽ trả về mảng chứa các lớp đầu ra dự đoán tương ứng với các véc-tơ đầu vào của `X`, lớp đầu ra dự đoán của một véc-tơ đầu vào là lớp mà có xác suất lớn nhất trong véc-tơ đầu ra chứa xác suất các lớp; mảng này có shape là (`N`,)\n",
    "\n",
    "Trong hàm `compute_nnet_output` thì bạn được phép dùng vòng lặp để duyệt qua các tầng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    return 1/(1+np.exp(-X))\n",
    "\n",
    "def softmax(z):\n",
    "    res = np.exp(z)/np.exp(z).sum(axis=1, keepdims=True)   \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c1df722b9884b005ae7c9b53b29fae0",
     "grade": false,
     "grade_id": "cell-a7098fc22ed23437",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_nnet_output(Ws, X, return_what='class'):\n",
    "    # YOUR CODE HERE\n",
    "    result = []\n",
    "    result.append(X)\n",
    "    for i in range(0, len(Ws)):\n",
    "        if(i!=len(Ws)-1): result.append(add_ones(sigmoid(result[i] @ Ws[i])))\n",
    "        else: result.append(softmax(result[i] @ Ws[i]))\n",
    "    if(return_what=='all'):\n",
    "        return result\n",
    "    if(return_what=='prob'):\n",
    "        return result[len(Ws)]\n",
    "    else: return result[len(Ws)].argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f1f6679f2e85bd8598f35e54188c5bf",
     "grade": true,
     "grade_id": "cell-12f18fd7316e849b",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "# Giả sử ta có mảng X với 4 dòng tương ứng với 4 véc-tơ đầu vào (đã thêm 1 ở đầu)\n",
    "X = np.array([[1.0, 0.9, 0.9], \n",
    "              [1.0, 0.5, 0.4], \n",
    "              [1.0, 0.4, 0.5],\n",
    "              [1.0, 0.1, 0.7]])\n",
    "\n",
    "# Giả sử Neural Network của ta có:\n",
    "# 2 nơ-ron đầu vào - 4 nơ-ron ẩn - 3 nơ-ron ẩn - 2 nơ-ron đầu ra\n",
    "# (không tính nơ-ron mà luôn có giá trị đầu ra là 1)\n",
    "Ws = [np.array([[-0.3 ,  0.2 ,  0.5 , -0.6],\n",
    "                [-0.1 , -0.2 , -0.35,  0.1],\n",
    "                [ 0.45, -0.7 , -0.7 ,  0.9]]),\n",
    "      np.array([[ 0.3 , -0.05,  0.8],\n",
    "                [ 0.6 ,  0.3 , -0.5],\n",
    "                [-0.8 , -0.3 ,  0.5],\n",
    "                [ 0.4 , -0.45,  0.4],\n",
    "                [ 0.1 ,  0.2 , -0.3]]),\n",
    "      np.array([[-0.3 ,  0.6],\n",
    "                [ 0.5 , -0.7],\n",
    "                [-0.45,  0.2],\n",
    "                [ 0.47, -0.2]])]\n",
    "\n",
    "# Kiểm tra hàm compute_nnet_output của bạn!\n",
    "As = compute_nnet_output(Ws, X, 'all')\n",
    "assert len(As) == 4\n",
    "assert As[0].shape == (4, 3)\n",
    "assert As[1].shape == (4, 5)\n",
    "assert As[2].shape == (4, 4)\n",
    "assert As[3].shape == (4, 2)\n",
    "assert str(np.round(As[0][0, 1], 4)) == '0.9'\n",
    "assert str(np.round(As[1][0, 1], 4)) == '0.5037'\n",
    "assert str(np.round(As[2][0, 1], 4)) == '0.6305'\n",
    "assert str(np.round(As[3][0, 1], 4)) == '0.5022'\n",
    "predicted_Y = compute_nnet_output(Ws, X, 'prob')\n",
    "assert predicted_Y.shape == (4, 2)\n",
    "assert str(np.round(predicted_Y[0, 1], 4)) == '0.5022'\n",
    "predicted_y = compute_nnet_output(Ws, X, 'class')\n",
    "assert predicted_y.shape == (4,)\n",
    "assert list(predicted_y) == [1, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau khi đã xong nhiệm vụ 1 thì bây giờ bạn sẽ sang nhiệm vụ 2 và cũng là nhiệm vụ chính của bạn (4đ): viết hàm `train_nnet` để huấn luyện mô hình Neural Network với độ lỗi cross-entropy và thuật toán cực tiểu hóa SGD (Stochastic Gradient Descent).\n",
    "\n",
    "Hàm `train_nnet` có các tham số đầu vào:\n",
    "- `X`, `y`, `initial_Ws`, `mb_size`, `lr`, `max_epoch`: giống như ở HW3, mình sẽ không nhắc lại nữa. Một khác biệt nhỏ so với HW3 là `initial_W` được thay bằng `initial_Ws` (có s), vì bây giờ ta không chỉ có một mảng trọng số mà là một list các mảng trọng số. Ngoài ra, so với HW3 thì ở đây ta chỉ dùng `max_epoch` để ngắt SGD, chứ không có `wanted_mbe` (ở HW3, ta đưa thêm `wanted_mbe` chủ yếu là để so sánh GD và SGD)\n",
    "- `hid_layer_sizes`: list cho biết số lượng nơ-ron của các tầng ẩn (không tính nơ-ron mà luôn có giá trị đầu ra là 1); ví dụ, list `[20, 10]` nghĩa là Neural Network của ta có 2 tầng ẩn, trong đó tầng ẩn thứ nhất có 20 nơ-ron, tầng ẩn thứ hai có 10 nơ-ron\n",
    "\n",
    "Hàm `train_nnet` trả về:\n",
    "- List chứa các mảng trọng số của mô hình Neural Network. Phần tử chỉ số `l-1` của list này là mảng trọng số của tầng `l` với `l` >= 1 (tầng 0 là tầng đầu vào, không có mảng trọng số); mảng trọng số của tầng `l` có shape là <font color=blue>(</font>1 + số lượng nơ-ron của tầng `l-1`<font color=blue>,</font> số lượng nơ-ron của tầng `l`<font color=blue>)</font> với \"số lượng nơ-ron của tầng\" là không tính nơ-ron mà luôn có giá trị đầu ra là 1\n",
    "- List chứa độ lỗi cross-entropy trên dữ liệu huấn luyện sau mỗi epoch (giống HW3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như đã nói, để tìm các trọng số của mô hình Neural Network thì ta sẽ dùng thuật toán SGD để cực tiểu độ lỗi cross-entropy trên dữ liệu huấn luyện. Như vậy, về cơ bản là hàm `train_nnet` giống như hàm `train_smreg` ở HW3 (ở đây ta cũng thống nhất là sẽ sử dụng cách cài đặt bước \"xáo trộn ngẫu nhiên các mẫu dữ liệu huấn luyện\" và cách xử lý phần dữ liệu bị lẻ ở cuối giống như ở HW3). Có một điểm khác là ở mỗi mini-batch, ta không chỉ phải cập nhật cho các trọng số của mô hình Softmax Regression ứng với tầng đầu ra, mà còn phải cập nhật cho các trọng số của các tầng ẩn trước đó. \n",
    "\n",
    "Như đã biết, trong SGD, để cập nhật cho một trọng số thì ta cần tính trung bình trên mini-batch đạo hàm riêng của độ-lỗi-của-một-mẫu-dữ-liệu-huấn-luyện theo trọng số này. Xét trọng số nối từ nơ-ron i của tầng l-1 tới nơ-ron j của tầng l. Sau khi áp dụng \"chain rule\" (chi tiết bạn có thể xem trong [slide về Neural Network của mình](https://drive.google.com/file/d/1IH7nXTR-0kVxWyyDlNorlxBAyl0eWECb/view?usp=sharing)), ta có công thức để tính đạo hàm riêng của độ-lỗi-của-một-mẫu-dữ-liệu-huấn-luyện theo trọng số này:\n",
    "\n",
    "Đạo hàm riêng <font color=blue>=</font> giá trị đầu ra của nơ-ron i ở tầng l-1 <font color=blue>*</font> delta của nơ-ron j ở tầng l\n",
    "\n",
    "Với delta của một nơ-ron là đạo hàm riêng của độ-lỗi-của-một-mẫu-dữ-liệu-huấn-luyện theo tổng có trọng số (giá trị tính được ngay trước khi áp dụng hàm kích hoạt) của nơ-ron này."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dưới đây là mã giả của thuật toán SGD để huấn luyện mô hình Neural Network.\n",
    "\n",
    "===\n",
    "\n",
    "Khởi tạo các trọng số (lưu ý: với Neural Network, cách khởi tạo trọng số sẽ ảnh hưởng nhiều đến kết quả tìm được bởi SGD)\n",
    "\n",
    "Với mỗi epoch (một epoch ám chỉ một lần duyệt qua toàn bộ các mẫu dữ liệu huấn luyện):\n",
    "- Xáo trộn ngẫu nhiên các mẫu dữ liệu huấn luyện\n",
    "- Chia các mẫu dữ liệu huấn luyện ra thành các mini-batch có kích thước bằng nhau (kích thước này do người dùng chỉ định). Với mỗi mini-batch:\n",
    "    1. Thực hiện lan truyền tiến các véc-tơ đầu vào qua Neural Network để có được các véc-tơ đầu ra ở tất cả các tầng\n",
    "    2. Tính các véc-tơ delta của tầng cuối từ các véc-tơ đầu ra của tầng cuối (véc-tơ chứa xác suất các lớp) và các véc-tơ đầu ra đúng của mini-batch (véc-tơ one-hot) \n",
    "    3. Tính gradient (các đạo riêng) của tầng cuối từ các véc-tơ đầu ra của tầng kế cuối và các véc-tơ delta của tầng cuối\n",
    "    4. Cập nhật các trọng số của tầng cuối dựa trên gradient\n",
    "    5. Duyệt từ tầng kế cuối về đầu. Với tầng i:\n",
    "        1. Tính các véc-tơ delta của tầng i từ các véc-tơ delta của tầng i+1, các trọng số của tầng i+1, và các véc-tơ đầu ra của tầng i\n",
    "        2. Tính gradient (các đạo riêng) của tầng i từ các véc-tơ đầu ra của tầng i-1 và các véc-tơ delta của tầng i\n",
    "        3. Cập nhật các trọng số của tầng i dựa trên gradient\n",
    "\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong mã giả ở trên, bạn đã cài đặt bước 1 ứng với hàm `compute_nnet_output` rồi. Bây giờ mình sẽ đưa ra các ví dụ cụ thể cho các bước 2, 3, 5A, 5B; dựa vào các ví dụ cụ thể này, bạn có thể kiểm tra xem code của bạn cho các bước này đã đúng hay chưa. Nói thêm: thật ra bạn đã làm bước 2 và 3 khi cài đặt mô hình Softmax Regression rồi, nhưng lúc đó bạn không tách ra bước 2.\n",
    "\n",
    "**Bước 2.** Tính các véc-tơ delta của tầng cuối từ các véc-tơ đầu ra của tầng cuối (véc-tơ chứa xác suất các lớp) và các véc-tơ đầu ra đúng của mini-batch (véc-tơ one-hot)\n",
    "\n",
    "Giả sử ta có 3 lớp và kích thước của mini-batch là 2. Gọi `A` là mảng 2 chiều chứa các véc-tơ đầu ra của tầng cuối (mỗi véc-tơ là một dòng của mảng) ứng với các véc-tơ đầu vào của mini-batch, và `mb_Y` là mảng 2 chiều chứa các véc-tơ đầu ra đúng của mini-batch (véc-tơ đầu ra đúng là véc-tơ one-hot, mỗi véc-tơ là một dòng của mảng). Nếu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[0.8, 0.7, 0.6],\n",
    "              [0.5, 0.6, 0.5]]) \n",
    "mb_Y = np.array([[0, 1, 0],\n",
    "                 [1, 0, 0]])\n",
    "A - mb_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thì mảng 2 chiều chứa các véc-tơ delta của tầng cuối (mỗi véc-tơ là một dòng của mảng) sẽ bằng:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[-0.8,  0.3, -0.6],\n",
    "          [ 0.5, -0.6, -0.5]]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để ra được cách tính mảng chứa các véc-tơ delta của tầng cuối thì bạn cần lấy giấy bút ra và tính đạo hàm riêng ... Trong trường hợp bạn thấy khó quá thì bạn có thể đoán cách tính từ ví dụ ở trên ;-) và hy vọng cách tính này sẽ là cách tính tổng quát."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bước 3.** Tính gradient (các đạo riêng) của tầng cuối từ các véc-tơ đầu ra của tầng kế cuối và các véc-tơ delta của tầng cuối\n",
    "\n",
    "Giả sử ta có 3 lớp (cũng tức là tầng cuối có 3 nơ-ron), tầng kế cuối có 4 nơ-ron (không tính nơ-ron mà luôn có giá trị đầu ra là 1), và kích thước của mini-batch là 2. Gọi `delta` là mảng 2 chiều chứa các véc-tơ delta của tầng cuối (mỗi véc-tơ là một dòng của mảng) ứng với các mẫu dữ liệu huấn luyện của mini-batch (mẫu dữ liệu huấn luyện gồm véc-tơ đầu vào và đầu ra đúng), và `A` là mảng 2 chiều chứa các véc-tơ đầu ra của tầng kế cuối (mỗi véc-tơ là một dòng của mảng) ứng với các véc-tơ đầu vào của mini-batch. Nếu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = np.array([[ 0.8, -0.3,  0.6],\n",
    "                  [-0.5,  0.6,  0.5]])\n",
    "A = np.array([[1.0, 0.5, 0.1, 0.3, 0.2],\n",
    "              [1.0, 0.9, 0.8, 0.7, 0.1]])\n",
    "grad = A.T @ delta / A.shape[0]\n",
    "print(grad.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thì gradient của tầng cuối (gradient là mảng có cùng shape với mảng trọng số của tầng cuối) sau khi gọi phương thức `round(4)` (làm tròn 4 chữ số thập phân) sẽ bằng:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[ 0.15 ,  0.15 ,  0.55 ],\n",
    "          [-0.025,  0.195,  0.375],\n",
    "          [-0.16 ,  0.225,  0.23 ],\n",
    "          [-0.055,  0.165,  0.265],\n",
    "          [ 0.055,  0.   ,  0.085]]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gợi ý: ở trên mình đã có đưa ra công thức để tính đạo hàm riêng của độ-lỗi-của-một-mẫu-dữ-liệu-huấn-luyện theo một trọng số; với một mini-batch thì bạn sẽ dùng công thức này để tính đạo hàm riêng với từng mẫu dữ liệu huấn luyện trong mini-batch, rồi sau cùng là lấy trung bình lại. \n",
    "\n",
    "Nếu thấy khó quá thì bạn có thể đoán cách tính gradient từ ví dụ ở trên và hy vọng ... Để củng cố hy vọng, bạn có thể đối chiếu với cách tính gradient của mô hình Softmax Regression mà bạn đã cài đặt ở HW3; như mình đã có nói, thật ra là bạn đã cài đặt bước 2 và 3 ở HW3, nhưng lúc đó là bạn không tách bước 2 ra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bước 5A.** Tính các véc-tơ delta của tầng i từ các véc-tơ delta của tầng i+1, các trọng số của tầng i+1, và các véc-tơ đầu ra của tầng i (lưu ý là ta không tính delta của nơ-ron mà luôn có giá trị đầu ra là 1) \n",
    "\n",
    "Giả sử tầng i có 4 nơ-ron, tầng i+1 có 3 nơ-ron (không tính nơ-ron mà luôn có giá trị đầu ra là 1), kích thước của mini-batch là 2. Gọi `delta` là mảng 2 chiều chứa các véc-tơ delta ở tầng i+1 (mỗi véc-tơ là một dòng của mảng) ứng với các mẫu dữ liệu huấn luyện của mini-batch, `W` là mảng 2 chiều chứa các trọng số của tầng i+1, `A` là mảng 2 chiều chứa các véc-tơ đầu ra của tầng i (mỗi véc-tơ là một dòng của mảng) ứng với các véc-tơ đầu vào của mini-batch. Nếu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = np.array([[ 0.8, -0.3,  0.6],\n",
    "                  [-0.5,  0.6,  0.5]])\n",
    "W = np.array([[-0.75,  0.95,  0.15],\n",
    "              [ 0.25,  0.29,  0.97],\n",
    "              [-0.56,  0.52,  0.73],\n",
    "              [ 0.05,  0.66,  0.46],\n",
    "              [-0.05,  0.  , 0.08]])\n",
    "A = np.array([[1.0, 0.5, 0.1, 0.3, 0.2],\n",
    "              [1.0, 0.9, 0.8, 0.7, 0.1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thì mảng 2 chiều chứa các véc-tơ delta của tầng i (mỗi véc-tơ là một dòng của mảng) sẽ được tính như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta.dot(W.T[:, 1:]) * A[:, 1:] * (1 - A[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do bước này hơi khó nên mình tiết lộ code luôn rồi đó ;-)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bước 5B.** Tính gradient (các đạo riêng) của tầng i từ các véc-tơ đầu ra của tầng i-1 và các véc-tơ delta của tầng i\n",
    "\n",
    "Tương tự như bước 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trước khi bạn viết hàm `train_nnet` thì mình nói thêm ý cuối cùng này nữa. Trong hàm `train_nnet` thì bạn được phép dùng vòng lặp để duyệt qua các epoch, trong mỗi epoch thì bạn được phép dùng vòng lặp để duyệt qua các mini-batch, trong mỗi mini-batch thì bạn được phép dùng vòng lặp để duyệt các tầng của Neural Network. Các bước 2, 3, 4, 5A, 5B, 5C trong mã giả ở trên đều có thể được cài đặt chỉ với một dòng code cho mỗi bước."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a932d02f08426bae87cc821fa82e840",
     "grade": false,
     "grade_id": "cell-aa53d3e85ae3e1ff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_nnet(X, y, \n",
    "               hid_layer_sizes, \n",
    "               initial_Ws, mb_size, lr, max_epoch):\n",
    "    # Cách để khởi tạo tốt các trọng số khá là \"bí hiểm\"\n",
    "    # nên mình sẽ làm cho bạn ;-)\n",
    "    np.random.seed(0) # Cố định sự ngẫu nhiên đề mình và bạn sẽ ra cùng kết quả\n",
    "    n_classes = len(np.unique(y)) \n",
    "    if initial_Ws is None:\n",
    "        layer_sizes = [X.shape[1] - 1] + hid_layer_sizes + [n_classes]\n",
    "        Ws = [np.random.randn(layer_sizes[i] + 1, layer_sizes[i + 1]) \n",
    "              / np.sqrt(layer_sizes[i] + 1) \n",
    "              for i in range(len(layer_sizes) - 1)] \n",
    "    else:\n",
    "        Ws = initial_Ws\n",
    "    \n",
    "    # Phần còn lại là của bạn\n",
    "    # YOUR CODE HERE\n",
    "    y_one_hot = np.zeros((len(y), 10))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    index_array = np.arange(y.shape[0])\n",
    "    cross_entropy = []\n",
    "\n",
    "    for i in range(max_epoch):\n",
    "        np.random.shuffle(index_array)\n",
    "        for j in range(int(len(index_array)/mb_size)):\n",
    "\n",
    "            arr_temp_X, arr_temp_y_one_hot = X[index_array[j*mb_size:(j+1)*mb_size]], y_one_hot[index_array[j*mb_size:(j+1)*mb_size]]\n",
    "            As = compute_nnet_output(Ws, arr_temp_X, 'all')\n",
    "            delta = As[len(As)-1] - arr_temp_y_one_hot\n",
    "            grad = As[len(As)-2].T @ delta / As[len(As)-2].shape[0]\n",
    "            Ws[len(Ws)-1] = Ws[len(Ws)-1] - lr*grad\n",
    "\n",
    "            delta_z = delta.copy()\n",
    "            \n",
    "            for z in range(len(layer_sizes)-2, 0, -1):\n",
    "                delta_z = delta_z.dot(Ws[z].T[:, 1:]) * As[z][:, 1:] * (1 - As[z][:, 1:])\n",
    "                grad_z = As[z-1].T @ delta_z / As[z-1].shape[0]\n",
    "                Ws[z-1] = Ws[z-1] - lr*grad_z\n",
    "\n",
    "        p_n = compute_nnet_output(Ws, X, 'prob')\n",
    "        grad = X.T @ (p_n-y_one_hot) / X.shape[0]\n",
    "        cross_entropy.append(-(np.sum(y_one_hot * np.log(p_n)))/ y_one_hot.shape[0])    \n",
    "\n",
    "    return Ws, cross_entropy;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aad1c4a6a216a6258646e2e81b8b26bd",
     "grade": true,
     "grade_id": "cell-02856e5a3b967345",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "Ws, ces = train_nnet(train_Z, train_y, \n",
    "                    hid_layer_sizes=[20], \n",
    "                    initial_Ws=None, mb_size=32, lr=0.3, max_epoch=2)\n",
    "assert len(Ws) == 2\n",
    "assert Ws[0].shape == (785, 20)\n",
    "assert Ws[1].shape == (21, 10)\n",
    "assert str(Ws[0][0, 0].round(4)) == '0.15'\n",
    "assert str(Ws[1][0, 0].round(4)) == '0.1992'\n",
    "assert len(ces) == 2\n",
    "assert str(np.round(ces[0], 4)) == '0.285'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ở trên, ta chỉ test nhỏ nhỏ để nhanh chóng kiểm tra tính đúng đắn của hàm `train_nnet`. Bây giờ, ta mới làm thật: gọi hàm `train_nnet` để tìm các trọng số của mô hình Neural Network từ `train_Z` và `train_y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Ws_1, train_ces = train_nnet(\n",
    "    train_Z, train_y,\n",
    "    hid_layer_sizes=[50], \n",
    "    initial_Ws=None, mb_size=32, lr=0.3, max_epoch=100)\n",
    "plt.plot(np.log(train_ces))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Log (for clarity) of training cross-entropy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dùng mô hình Neural Network tìm được để dự đoán với dữ liệu huấn luyện và đánh giá kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mbe(predicted_y, y):\n",
    "    return np.mean(predicted_y != y) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_train_y = compute_nnet_output(Ws_1, train_Z, return_what='class')\n",
    "train_mbe = compute_mbe(predicted_train_y, train_y)\n",
    "assert str(np.round(train_mbe, 4)) == '0.002'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ở HW3, độ lỗi MBE trên dữ liệu huấn luyện của mô hình Softmax Regression (cùng cách tiền xử lý) tốt nhất là 4.x%. Với mô hình Neural Network ở bài này, độ lỗi MBE trên dữ liệu huấn luyện đã giảm xuống 0.002%!\n",
    "\n",
    "Nhưng ta khoan hãy vui mừng, vì độ lỗi siêu thấp trên dữ liệu huấn luyện có thể là do \"học vẹt\". Ta hãy xem độ lỗi trên dữ liệu validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiền xử lý dữ liệu validation, dùng mô hình Neural Network tìm được để dự đoán với dữ liệu validation và đánh giá kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_Z = add_ones(val_X)\n",
    "predicted_val_y = compute_nnet_output(Ws_1, val_Z, return_what='class')\n",
    "val_mbe = compute_mbe(predicted_val_y, val_y)\n",
    "assert str(np.round(val_mbe, 4)) == '2.89'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Độ lỗi MBE trên dữ liệu validation cũng giảm theo (mặc dù không giảm nhiều như trên dữ liệu huấn luyện): từ 4.x% với mô hình Softmax Regression tốt nhất ở HW3 giảm xuống 2.89%! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Đưa ra ý tưởng cải tiến"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "026f1f51e388267c854c602d39a4f3a3",
     "grade": false,
     "grade_id": "cell-134c34ab9e8431ce",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    },
    "tags": []
   },
   "source": [
    "Nhiệm vụ của bạn (2đ): đưa ra ý tưởng để có thể cải tiến kết quả (trên dữ liệu validation) của mô hình Neural Network hiện tại. Khi đưa ra ý tưởng thì bạn cần giải thích để người đọc thấy được tại sao ý tưởng này lại có khả năng cải tiến kết quả hiện tại. Nếu ý tưởng mà bạn đưa ra được tham khảo hoặc lấy cảm hứng từ đâu thì bạn cũng cần ghi rõ nguồn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE \\\n",
    "(Nếu muốn thì bạn có thể chèn thêm các markdown cell để trình bày)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đây là một số cách cải tiến để tiền xử lý dữ liệu đầu vào trước khi đưa vào training của mô hình neural network mà em tìm hiểu được  (Nguồn: https://iopscience.iop.org/article/10.1088/1757-899X/1084/1/012021/pdf)\n",
    "\n",
    "- Binarization (Nhị phân hóa ảnh): Chuyển đổi một hình ảnh thành một hình ảnh nhị phân bằng cách ngưỡng các giá trị pixel. Kỹ thuật này có thể được sử dụng để giảm lượng dữ liệu cần được xử lý bởi mạng thần kinh và có thể giúp cải thiện độ chính xác của nó. Có nhiều phương pháp khác nhau để nhị phân hóa hình ảnh, bao gồm global thresholding, adaptive thresholding, và Otsu’s method \n",
    "\n",
    "- Noise elimination (Loại bỏ độ nhiễu): Loại bỏ nhiễu không mong muốn khỏi hình ảnh trước khi hình ảnh được xử lý bởi mạng nơ-ron. Có nhiều phương pháp khác nhau để loại bỏ nhiễu khỏi ảnh, bao gồm median filtering, Gaussian filtering và bilateral filtering. Việc lựa chọn phương pháp nào phụ thuộc vào loại nhiễu có trong ảnh và mức giảm nhiễu mong muốn\n",
    "\n",
    "- Skew detection (Loại bỏ độ nghiêng - Giống với HW03): Độ nghiêng của chữ viết cũng ảnh hướng rất lớn đến kết quả cuối cùng của mô hình.\n",
    "\n",
    "- Skeleton (Khung): Thông thường có sự đối lập với chiều rộng của đường từ vài pixel đến rộng một pixel. Trong trường hợp này, quá trình khung xương có thể đề cập đến việc giảm độ rộng của dòng bằng cách loại bỏ nhiều điểm khác biệt, để đơn giản hóa thuật toán phân loại và đồng thời giảm thời gian xử lý\n",
    "\n",
    "Tuy vậy, vẫn còn 1 vấn đề là dữ liệu training có độ lỗi MBE là 0.002%, điều này có thể đến vấn đề overfitting (dữ liệu quá khợp với bộ training và đạt kết quả không tốt đối với bộ validation và test). Để giảm bớt vấn đề này thì em sẽ giảm max_epoch đi 1 nửa là từ 100 xuống còn 50, bộ trọng số có thể sẽ không được cực tiểu hóa nhiều như khi thực hiện với max_epoch = 100 nhưng kết quả đạt được ở đây là thời gian chạy nhanh hơn và bộ dữ liệu validation và test cho ra kết quả tốt hơn.\n",
    "\n",
    "Đồng thời số lượng noron cũng ảnh hưởng đến kết quả đầu ra, do dữ liệu đầu vào khá lớn nhưng noron của tầng ẩn chỉ có 50 nên kết quả sẽ không chính xác => Em sẽ tăng số lượng noron của tầng ẩn lên 100 đối với phương pháp tiền xử lý để nhận được độ lỗi mbe thấp hơn\n",
    "\n",
    "Ngoài ra, các hàm activation cũng sẽ dẫn tới các kết quả đầu ra cũng như độ chính xác của mô hình.\n",
    "- Hàm kích hoạt (activation function) mô phỏng tỷ lệ truyền xung qua axon của một neuron thần kinh. Trong một mạng nơ-ron nhân tạo, hàm kích hoạt đóng vai trò là thành phần phi tuyến tại output của các nơ-ron ( Định nghĩa này lấy từ nguồn: https://aicurious.io/blog/2019-09-23-cac-ham-kich-hoat-activation-function-trong-neural-networks)\n",
    "- Các hàm activation phổ biến trong mạng neural nhân tạo bao gồm: sigmoid, ReLU (Rectified Linear Unit), tanh (hyperbolic tangent), Leaky ReLU và Softmax\n",
    "\n",
    "=> Ở đây ngoài hàm sigmoid như đã thực hiện ở trên em sẽ thay thế hàm activation bằng ReLU và tanh để xem độ lỗi MBE cải thiện như thế nào"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf401db2f4b6fb1d6c9ed16ddda9894d",
     "grade": false,
     "grade_id": "cell-2054bd26c1337331",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    },
    "tags": []
   },
   "source": [
    "Nhiệm vụ kế tiếp của bạn (2đ): cài đặt ý tưởng cải tiến của bạn! Ở bên dưới, mình đã để sẵn các mục quen thuộc cho bạn, với mỗi mục thì bạn có thể tùy ý chèn thêm các cell để làm. \n",
    "\n",
    "Nếu cải tiến của bạn không liên quan đến việc tiền xử lý dữ liệu thì bạn có thể bỏ qua mục \"Tiền xử lý dữ liệu huấn luyện\". \n",
    "\n",
    "Ở mục \"Tiền xử lý dữ liệu validation, dùng mô hình Neural Network tìm được để dự đoán với dữ liệu validation và đánh giá kết quả\", bạn nhớ in ra độ lỗi MBE trên dữ liệu validation và so sánh với độ lỗi MBE hiện tại (2.89%); trong trường hợp kết quả không được cải thiện, bạn thử suy nghĩ xem tại sao lại như vậy, nếu không biết tại sao thì bạn cứ nói là không biết tại sao. \n",
    "\n",
    "Hai mục sau cùng (\"Chọn ra cách tiền xử lý + mô hình Neural Network sau cùng là cách tiền xử lý + mô hình Neural Network mà có độ lỗi dự đoán thấp nhất trên dữ liệu validation\" và \"Dùng cách tiền xử lý + mô hình Neural Network sau cùng để đi thi thật!\") là hai mục không bắt buộc (không có điểm) nhưng bạn nên làm cho đầy đủ. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiền xử lý dữ liệu huấn luyện"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HÀM NHỊ PHÂN HÓA**\n",
    "\n",
    "Ý tưởng thuật toán này là những điểm ảnh (pixel) nào có giá trị lớn hơn threshold thì sẽ trở thành 1 (điểm ảnh thành màu trắng), ngược lại sẽ bằng 0 (điểm ảnh thành màu đen). Ở đây có nhiều cách để tìm threshold:\n",
    "- Thuật toán Otsu (Thuật toán Otsu là một phương pháp ngưỡng phổ biến giả định hình ảnh chứa hai lớp pixel - nền trước và nền và có biểu đồ hai chế độ. Sau đó, nó cố gắng giảm thiểu sự lây lan kết hợp của chúng (phương sai trong lớp) - Nguồn: https://qastack.vn/signals/2411/what-are-the-most-common-algorithms-for-adaptive-thresholding)\n",
    "- Ngưỡng trung bình (average intensity)\n",
    "- Ngưỡng trung vị (median intensity)\n",
    "- Ngưỡng trung bình cộng k lần độ lệch chuẩn của độ sáng (average + k * standard deviation of intensity)\n",
    "- Ngưỡng trung vị cộng k lần độ lệch tuyệt đối của trung vị (median + k * median absolute deviation)\n",
    "\n",
    "Tuy nhiên đối với HW03 do đã thống nhất \"Nếu giá trị pixel > 0.5 thì ta sẽ xem đây là pixel của chữ số\" nên em sẽ chọn threshold = 0.5 cho đồng nhất với thuật toán loại bỏ độ nghiêng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToOneOrZero(X):\n",
    "    result=np.zeros([X.shape[0], X.shape[1]])\n",
    "    threshold = 0.5\n",
    "    for i in range(X.shape[0]):\n",
    "        result[i] = np.where(X[i]>threshold, 1, 0)     \n",
    "    return result\n",
    "\n",
    "train_binary = add_ones(convertToOneOrZero(train_X))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HÀM LOẠI BỎ ĐỘ NGHIÊNG**\n",
    "\n",
    "Ý tưởng thuật toán này lấy từ HW3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertOneNumber(I):\n",
    "    rr=np.where(I>0.5)[0]\n",
    "    cc=np.where(I>0.5)[1]\n",
    "    s_rr = (rr - rr.mean()) / rr.std()\n",
    "    s_cc = (cc - cc.mean()) / cc.std()\n",
    "    corr_coef = np.mean(s_rr*s_cc)\n",
    "    a = corr_coef * cc.std() / rr.std()\n",
    "    r_coor = rr.mean()\n",
    "\n",
    "    none_italic = np.empty([28,28])\n",
    "\n",
    "    for i in range(I.shape[0]):\n",
    "        for j in range(I.shape[1]):\n",
    "            ic = j - (r_coor-i)*a \n",
    "            floor_val = I[i][min(max(0,int(np.floor(ic))),I.shape[1]-1)]\n",
    "            ceil_val = I[i][min(max(0,int(np.ceil(ic))),I.shape[1]-1)]\n",
    "            con = np.ceil(ic) - ic\n",
    "            none_italic[i][j]=con*floor_val + (1.0-con)*ceil_val\n",
    "\n",
    "    return none_italic\n",
    "\n",
    "def deskew(X):\n",
    "    # YOUR CODE HERE\n",
    "    result=np.zeros([X.shape[0], X.shape[1]])\n",
    "    for x in range(X.shape[0]):\n",
    "        I = X[x].reshape(28, 28)\n",
    "        result[x]=convertOneNumber(I).ravel()       \n",
    "    return result\n",
    "\n",
    "train_skew = add_ones(deskew(train_X))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do bộ dữ liệu MNIST đã tiền xử lý trước đó nên các thuật toán noise elimination sẽ không cần thiết nên em sẽ không viết hàm xử lý"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tìm mô hình Neural Network từ dữ liệu huấn luyện"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em sẽ tiến hành thử nghiệm lần lượt các hàm activation là linear, tanh và ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(X):\n",
    "    return np.tanh(X)\n",
    "\n",
    "def ReLU(X):\n",
    "    return np.maximum(X, 0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ở đây em sẽ chỉnh sửa 1 chút ở hàm train_nnet và compute_nnet_output là train_nnet_ver_2 và compute_nnet_output_ver_2 (Em thêm 1 tham số đầu vào là type_act, type_act có giá trị là \"linear\" thì mô hình này sẽ kích hoạt hàm activation là linear, tương tự với type_act=\"tanh\" và type_act=\"ReLU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nnet_output_ver_2(Ws, X, return_what='class', type_act='tanh'):\n",
    "    # YOUR CODE HERE\n",
    "    result = []\n",
    "    result.append(X)\n",
    "    for i in range(0, len(Ws)):\n",
    "        if(i!=len(Ws)-1): \n",
    "            if(type_act==\"tanh\"): result.append(add_ones(tanh(result[i] @ Ws[i])))\n",
    "            if(type_act==\"relu\"): result.append(add_ones(ReLU(result[i] @ Ws[i])))\n",
    "        else: result.append(softmax(result[i] @ Ws[i]))\n",
    "    if(return_what=='all'):\n",
    "        return result\n",
    "    if(return_what=='prob'):\n",
    "        return result[len(Ws)]\n",
    "    else: return result[len(Ws)].argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nnet_ver_2(X, y, \n",
    "               hid_layer_sizes, \n",
    "               initial_Ws, mb_size, lr, max_epoch, type_act='aaa'):\n",
    "    # Cách để khởi tạo tốt các trọng số khá là \"bí hiểm\"\n",
    "    # nên mình sẽ làm cho bạn ;-)\n",
    "    np.random.seed(0) # Cố định sự ngẫu nhiên đề mình và bạn sẽ ra cùng kết quả\n",
    "    n_classes = len(np.unique(y)) \n",
    "    if initial_Ws is None:\n",
    "        layer_sizes = [X.shape[1] - 1] + hid_layer_sizes + [n_classes]\n",
    "        Ws = [np.random.randn(layer_sizes[i] + 1, layer_sizes[i + 1]) \n",
    "              / np.sqrt(layer_sizes[i] + 1) \n",
    "              for i in range(len(layer_sizes) - 1)] \n",
    "    else:\n",
    "        Ws = initial_Ws\n",
    "    \n",
    "    # Phần còn lại là của bạn\n",
    "    # YOUR CODE HERE\n",
    "    y_one_hot = np.zeros((len(y), 10))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    index_array = np.arange(y.shape[0])\n",
    "    cross_entropy = []\n",
    "\n",
    "    for i in range(max_epoch):\n",
    "        np.random.shuffle(index_array)\n",
    "        for j in range(int(len(index_array)/mb_size)):\n",
    "\n",
    "            arr_temp_X, arr_temp_y_one_hot = X[index_array[j*mb_size:(j+1)*mb_size]], y_one_hot[index_array[j*mb_size:(j+1)*mb_size]]\n",
    "            As = compute_nnet_output_ver_2(Ws, arr_temp_X, 'all', type_act)\n",
    "            delta = As[len(As)-1] - arr_temp_y_one_hot\n",
    "            grad = As[len(As)-2].T @ delta / As[len(As)-2].shape[0]\n",
    "            Ws[len(Ws)-1] = Ws[len(Ws)-1] - lr*grad\n",
    "\n",
    "            delta_z = delta.copy()\n",
    "            \n",
    "            for z in range(len(layer_sizes)-2, 0, -1):\n",
    "                delta_z = delta_z.dot(Ws[z].T[:, 1:]) * As[z][:, 1:] * (1 - As[z][:, 1:])\n",
    "                grad_z = As[z-1].T @ delta_z / As[z-1].shape[0]\n",
    "                Ws[z-1] = Ws[z-1] - lr*grad_z\n",
    "\n",
    "        p_n = compute_nnet_output_ver_2(Ws, X, 'prob', type_act)\n",
    "        grad = X.T @ (p_n-y_one_hot) / X.shape[0]\n",
    "        cross_entropy.append(-(np.sum(y_one_hot * np.log(p_n)))/ y_one_hot.shape[0])    \n",
    "\n",
    "    return Ws, cross_entropy;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiếp theo, em tiến hành train mô hình theo các phương pháp trên để nhận các bộ trọng số tương ứng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bộ trọng số của phương pháp tiền xử lý nhị phân hóa\n",
    "W_binary, train_ces_binary = train_nnet(\n",
    "    train_binary, train_y,\n",
    "    hid_layer_sizes=[100], \n",
    "    initial_Ws=None, mb_size=32, lr=0.3, max_epoch=50)\n",
    "\n",
    "#Bộ trọng số của phương pháp tiền xử lý loại bỏ độ nghiêng\n",
    "W_skew, train_ces_skew = train_nnet(\n",
    "    train_skew, train_y,\n",
    "    hid_layer_sizes=[100], \n",
    "    initial_Ws=None, mb_size=32, lr=0.3, max_epoch=50)\n",
    "\n",
    "#Bộ trọng số của phương pháp loại sử dụng hàm tanh activation\n",
    "W_tanh, train_ces_tanh = train_nnet_ver_2(\n",
    "    train_Z, train_y,\n",
    "    hid_layer_sizes=[50], \n",
    "    initial_Ws=None, mb_size=32, lr=0.3, max_epoch=50, type_act='tanh')\n",
    "\n",
    "#Bộ trọng số của phương pháp loại sử dụng hàm ReLU activation\n",
    "W_relu, train_ces_relu = train_nnet_ver_2(\n",
    "    train_Z, train_y,\n",
    "    hid_layer_sizes=[50], \n",
    "    initial_Ws=None, mb_size=32, lr=0.3, max_epoch=50, type_act='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiền xử lý dữ liệu validation, dùng mô hình Neural Network tìm được để dự đoán với dữ liệu validation và đánh giá kết quả"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phương pháp tiền xử lý nhị phân hóa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_Z = add_ones(convertToOneOrZero(val_X))\n",
    "predicted_val_y = compute_nnet_output(W_binary, val_Z, return_what=\"class\")\n",
    "val_mbe = compute_mbe(predicted_val_y, val_y)\n",
    "print(\"MBE trước khi cải tiến: \", '2.89')\n",
    "print(\"MBE sau khi cải tiến: \", val_mbe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phương pháp tiền xử lý loại bỏ độ nghiêng**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_Z = add_ones(deskew(val_X))\n",
    "predicted_val_y = compute_nnet_output(W_skew, val_Z, return_what=\"class\")\n",
    "val_mbe = compute_mbe(predicted_val_y, val_y)\n",
    "print(\"MBE trước khi cải tiến: \", '2.89')\n",
    "print(\"MBE sau khi cải tiến: \", val_mbe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phương pháp sử dụng hàm tanh activation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_Z = add_ones(val_X)\n",
    "predicted_val_y = compute_nnet_output(W_tanh, val_Z, return_what=\"class\")\n",
    "val_mbe = compute_mbe(predicted_val_y, val_y)\n",
    "print(\"MBE trước khi cải tiến: \", '2.89')\n",
    "print(\"MBE sau khi cải tiến: \", val_mbe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phương pháp sử dụng hàm ReLU activation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_val_y = compute_nnet_output(W_relu, val_Z, return_what=\"class\")\n",
    "val_mbe = compute_mbe(predicted_val_y, val_y)\n",
    "print(\"MBE trước khi cải tiến: \", '2.89')\n",
    "print(\"MBE sau khi cải tiến: \", val_mbe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NHẬN XÉT**\n",
    "\n",
    "Ở đây em nhận thấy chỉ có phương pháp tiền xử lý loại bỏ độ nghiêng sẽ cho ra kết quả có MBE thấp nhất, phương pháp nhị phân hóa có độ lỗi giảm ít và các phương pháp khác kết quả MBE không đạt như mong đợi:\n",
    "- Độ nghiêng có thể làm giảm độ chính xác của các thuật toán xử lý ảnh và làm giảm hiệu suất của chúng => Loại bỏ nó sẽ đạt kết quả tốt hơn\n",
    "- Phương pháp nhị phân hóa cho ra kết quả có tuy có giảm nhưng rất ít có thể là do nhiều chữ số có các nét chữ sau khi nhị phân hóa sẽ trở nên quá dày hoặc quá mỏng (tùy threshold) dẫn đến việc khó khăn trong nhận dạng chữ số\n",
    "- Phương pháp sử dụng hàm tanh activation có thể không phù hợp bởi vì Hàm tanh activation được sử dụng trong mạng neural để giới hạn giá trị đầu ra trong khoảng [-1, 1]. Nó có đạo hàm tốt hơn hàm sigmoid và cho phép mô hình học nhanh hơn. Tuy nhiên, nhược điểm của hàm tanh là khi đầu vào có trị tuyệt đối lớn (rất âm hoặc rất dương), gradient của hàm số này sẽ rất nhỏ gần bằng 0, dẫn đến hiện tượng vanishing gradient. (Nguồn: https://www.noron.vn/post/activation-functions-trong-neural-network-40ww2cdtly1f)\n",
    "- Phương pháp sử dụng hàm ReLU activation không phù hợp vì bới các node có giá trị nhỏ hơn 0, qua ReLU activation sẽ thành 0, hiện tượng đấy gọi là “Dying ReLU“. Nếu các node bị chuyển thành 0 thì sẽ không có ý nghĩa với bước linear activation ở lớp tiếp theo và các hệ số tương ứng từ node đấy cũng không được cập nhật với gradient descent. Khi learning rate lớn, các trọng số (weights) có thể thay đổi theo cách làm tất cả neuron dừng việc cập nhật (Nguồn: https://aicurious.io/blog/2019-09-23-cac-ham-kich-hoat-activation-function-trong-neural-networks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chọn ra cách tiền xử lý + mô hình Neural Network sau cùng là cách tiền xử lý + mô hình Neural Network mà có độ lỗi dự đoán thấp nhất trên dữ liệu validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em sẽ chọn cách tiền xử lý `add_ones(deskew(...))` (Loại bỏ độ nghiêng) và mô hình Neural Network có bộ trọng số `W_skew`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dùng cách tiền xử lý + mô hình Neural Network sau cùng để đi thi thật!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Z = add_ones(deskew(test_X))\n",
    "predicted_test_y = compute_nnet_output(W_skew, test_Z, return_what=\"class\")\n",
    "test_mbe = compute_mbe(predicted_test_y, test_y)\n",
    "print(test_mbe)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "BT05-NeuralNet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "103px",
    "width": "252px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "509px",
    "left": "0px",
    "right": "1212px",
    "top": "106px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
